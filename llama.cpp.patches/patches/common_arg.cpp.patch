diff --git a/common/arg.cpp b/common/arg.cpp
index 4316917d..a6333497 100644
--- a/llama.cpp/common/arg.cpp
+++ b/llama.cpp/common/arg.cpp
@@ -41,6 +41,10 @@
 
 #ifdef __linux__
 #include <linux/limits.h>
+// mmojo-server START
+#elif defined(COSMOCC)
+#include <linux/limits.h>
+// mmojo-server END
 #elif defined(_WIN32)
 #   if !defined(PATH_MAX)
 #   define PATH_MAX MAX_PATH
@@ -4123,5 +4127,25 @@ common_params_context common_params_parser_init(common_params & params, llama_ex
         }
     ).set_examples({LLAMA_EXAMPLE_SERVER}));
 
+    // mmojo-server START
+    // This could be automated by searching for "return ctx_arg" and inserting this block with newline padding directly before. -Brad 2025-11-05
+    add_opt(common_arg(
+        {"--default-ui-endpoint"}, "STRING",
+        "endpoint for accessing the default chat user interface",
+        [](common_params & params, const std::string & value) {
+            params.default_ui_endpoint = value;
+            // LOG_INF("Setting params.default_ui_endpoint: %s\n", params.default_ui_endpoint.c_str());
+        }
+    ).set_examples({LLAMA_EXAMPLE_SERVER}));
+
+    add_opt(common_arg(
+        {"--batch-sleep-ms"}, "N",
+        "sleep time in milliseconds after processing each batch; to keep CPUs and GPUs cool.",
+        [](common_params & params, int value) {
+            params.n_batch_sleep_ms = value;
+        }
+    ).set_examples({LLAMA_EXAMPLE_SERVER}));
+    // mmojo-server END
+
     return ctx_arg;
 }
